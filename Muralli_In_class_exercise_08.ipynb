{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "Muralli_In_class_exercise_08.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muraliRoyal1/venkatasiva_INFO5731_Fall2020/blob/master/Muralli_In_class_exercise_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWaxtLfVtECt"
      },
      "source": [
        "# **The eighth in-class-exercise (20 points in total, 10/29/2020)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-GW6VrFtECu"
      },
      "source": [
        "The data for this exercise is from the dataset you created from assignment three. Please perform answer the following questions based on your data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i8PEtUAtECv"
      },
      "source": [
        "## (1) (10 points) Write a python program to extract the sentiment related terms from the corpus. You may use python package such as polyglot or external lexicon resources in the question. Rank the sentiment related terms by frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVcpP9BHtECw",
        "outputId": "5ae1811e-e30e-4db6-de63-5ddf23f575a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "from textblob import TextBlob\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5b1mNzOucs9",
        "outputId": "226b9827-b951-41e7-969e-b977d68110ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dd = pd.read_csv(\"/content/sample_data/murallidata.csv\")\n",
        "dd = dd[['abs']]\n",
        "dd['abs'] = dd['abs'].str.replace('[^\\w\\s]','')\n",
        "dd['abs'] = dd['abs'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "dd['abs'] = dd['abs'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "dd['abs'] = dd['abs'].apply(lambda x: nltk.word_tokenize(x))\n",
        "values = (dd['abs']).apply(lambda x: pd.value_counts(x)).sum(axis = 0).reset_index()\n",
        "values.columns = ['Words_List', 'tf']\n",
        "values['polarity_Value'] = values['Words_List'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "terms = values.loc[values['polarity_Value'] != 0].sort_values(by='tf', ascending=False)\n",
        "terms = terms.reset_index(drop=True)\n",
        "terms.head(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Words_List</th>\n",
              "      <th>tf</th>\n",
              "      <th>polarity_Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>natural</td>\n",
              "      <td>71.0</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>linguistic</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>new</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.136364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>many</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>effective</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>developed</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>significant</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>limited</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.071429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>general</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.050000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wide</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>single</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-0.071429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>important</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>large</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.214286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>powerful</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>complex</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>first</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>available</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>artificial</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>useful</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>high</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.160000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>sensitive</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>little</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.187500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>less</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>able</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>mildly</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>naturally</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>popular</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>relevant</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>typically</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>unfortunately</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>usually</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>particular</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>widely</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>early</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>base</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>common</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>detailed</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>educational</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>better</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>due</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>capable</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>strongly</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.433333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>real</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>behind</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>raw</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.230769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>sophisticated</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>special</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.357143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>stereotypical</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>previous</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>best</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Words_List    tf  polarity_Value\n",
              "0         natural  71.0        0.100000\n",
              "1      linguistic  10.0        0.100000\n",
              "2             new   8.0        0.136364\n",
              "3            many   7.0        0.500000\n",
              "4       effective   7.0        0.600000\n",
              "5       developed   5.0        0.100000\n",
              "6     significant   4.0        0.375000\n",
              "7         limited   4.0       -0.071429\n",
              "8         general   4.0        0.050000\n",
              "9            wide   4.0       -0.100000\n",
              "10         single   3.0       -0.071429\n",
              "11      important   3.0        0.400000\n",
              "12          large   3.0        0.214286\n",
              "13       powerful   3.0        0.300000\n",
              "14        complex   3.0       -0.300000\n",
              "15          first   3.0        0.250000\n",
              "16      available   3.0        0.400000\n",
              "17     artificial   3.0       -0.600000\n",
              "18         useful   3.0        0.300000\n",
              "19           high   3.0        0.160000\n",
              "20      sensitive   3.0        0.100000\n",
              "21         little   2.0       -0.187500\n",
              "22           less   2.0       -0.166667\n",
              "23           able   2.0        0.500000\n",
              "24         mildly   2.0        0.333333\n",
              "25      naturally   2.0        0.100000\n",
              "26        popular   2.0        0.600000\n",
              "27       relevant   2.0        0.400000\n",
              "28      typically   2.0       -0.166667\n",
              "29  unfortunately   2.0       -0.500000\n",
              "30        usually   2.0       -0.250000\n",
              "31     particular   2.0        0.166667\n",
              "32         widely   2.0       -0.100000\n",
              "33          early   2.0        0.100000\n",
              "34           base   2.0       -0.800000\n",
              "35         common   2.0       -0.300000\n",
              "36       detailed   2.0        0.400000\n",
              "37    educational   2.0        0.250000\n",
              "38         better   2.0        0.500000\n",
              "39            due   2.0       -0.125000\n",
              "40        capable   1.0        0.200000\n",
              "41       strongly   1.0        0.433333\n",
              "42           real   1.0        0.200000\n",
              "43         behind   1.0       -0.400000\n",
              "44            raw   1.0       -0.230769\n",
              "45  sophisticated   1.0        0.500000\n",
              "46        special   1.0        0.357143\n",
              "47  stereotypical   1.0       -0.500000\n",
              "48       previous   1.0       -0.166667\n",
              "49           best   1.0        1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJrI7U1OtEC1"
      },
      "source": [
        "## (2) (10 points) Compare the performance of the following tools in sentiment identification: TextBlob (https://textblob.readthedocs.io/en/dev/), VADER (https://github.com/cjhutto/vaderSentiment), TFIDF-based Support Vector Machine (SVM) (Split your data into training and testing data). Take your own annotation as the standard answers. \n",
        "\n",
        "Reference code: https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLRS6S5ZtEC2",
        "outputId": "3dbf446f-5441-4a1b-c584-6b45cce2f915",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "dd1 = pd.read_csv(\"/content/sample_data/murallidata1 .csv\")\n",
        "dd1['polarity'] = dd1['abs'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "dd1['pred sen'] = pd.cut(dd1['polarity'], bins=3, labels=[1, 2, 3])\n",
        "def sentiment(x):\n",
        "    if x == 1:\n",
        "        return 'Positive'\n",
        "    if x == 2:\n",
        "        return 'Neutral'\n",
        "    if x ==3:\n",
        "        return 'Negative'\n",
        "dd1['pred sen'] = dd1['pred sen'].apply(lambda x: sentiment(x))\n",
        "print(dd1[[ 'sen', 'pred sen']].head(50))\n",
        "textblob_accuracy = accuracy_score(dd1['sen'], dd1['pred sen'])*100\n",
        "textblob_f1 = f1_score(dd1['sen'], dd1['pred sen'], average='macro')\n",
        "vader = SentimentIntensityAnalyzer()\n",
        "\n",
        "dd3 = pd.read_csv(\"/content/sample_data/murallidata1 .csv\")\n",
        "dd3['polarity'] = dd1['abs'].apply(lambda x: vader.polarity_scores(x)['compound'])\n",
        "dd3['pred sen'] = pd.cut(dd3['polarity'], bins=3, labels=[1, 2, 3])\n",
        "\n",
        "dd3['pred sen'] = dd3['pred sen'].apply(lambda x: sentiment(x))\n",
        "print(dd3[['sen', 'pred sen']].head(50))\n",
        "\n",
        "vader_accuracy = accuracy_score(dd3['sen'], dd3['pred sen'])*100\n",
        "vader_f1 = f1_score(dd3['sen'], dd3['pred sen'], average='macro')\n",
        "\n",
        "dd4 = pd.read_csv(\"/content/sample_data/murallidata1 .csv\")\n",
        "train, test = sklearn.model_selection.train_test_split(dd4, train_size=0.5, test_size=0.2)\n",
        "pipeline = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=100, \n",
        "                                           learning_rate='optimal', tol=None))])\n",
        "svm = pipeline.fit(train['abs'], train['sen'])\n",
        "test['pred sen'] = svm.predict(test['abs'])\n",
        "print(test[[ 'sen', 'pred sen']].head(50))\n",
        "svm_accuracy = accuracy_score(test['sen'], test['pred sen'])*100\n",
        "svm_f1 = f1_score(test['sen'], test['pred sen'], average='macro')\n",
        "print( svm_f1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "         sen  pred sen\n",
            "0   positive   Neutral\n",
            "1   positive   Neutral\n",
            "2   positive   Neutral\n",
            "3   positive  Negative\n",
            "4   positive  Negative\n",
            "5   positive   Neutral\n",
            "6   positive   Neutral\n",
            "7   positive   Neutral\n",
            "8   positive   Neutral\n",
            "9   positive   Neutral\n",
            "10  positive   Neutral\n",
            "11  positive  Negative\n",
            "12  positive  Negative\n",
            "13  positive  Negative\n",
            "14  positive   Neutral\n",
            "15  positive   Neutral\n",
            "16  negative  Negative\n",
            "17  positive   Neutral\n",
            "18  positive   Neutral\n",
            "19  positive   Neutral\n",
            "20  negative  Negative\n",
            "21  positive  Negative\n",
            "22  positive   Neutral\n",
            "23  positive   Neutral\n",
            "24  positive   Neutral\n",
            "25  positive   Neutral\n",
            "26  positive   Neutral\n",
            "27  positive   Neutral\n",
            "28  positive   Neutral\n",
            "29  positive   Neutral\n",
            "30  positive  Negative\n",
            "31  negative   Neutral\n",
            "32  positive   Neutral\n",
            "33  positive   Neutral\n",
            "34  positive   Neutral\n",
            "35  positive   Neutral\n",
            "36  positive   Neutral\n",
            "37  positive   Neutral\n",
            "38  positive   Neutral\n",
            "39  positive   Neutral\n",
            "40  positive   Neutral\n",
            "41  positive   Neutral\n",
            "42  positive  Negative\n",
            "43  positive   Neutral\n",
            "44  positive   Neutral\n",
            "45  positive   Neutral\n",
            "46  positive   Neutral\n",
            "47  positive  Negative\n",
            "48  positive   Neutral\n",
            "49  positive   Neutral\n",
            "         sen  pred sen\n",
            "0   positive  Negative\n",
            "1   positive  Negative\n",
            "2   positive  Negative\n",
            "3   positive  Negative\n",
            "4   positive   Neutral\n",
            "5   positive  Negative\n",
            "6   positive  Negative\n",
            "7   positive  Negative\n",
            "8   positive   Neutral\n",
            "9   positive  Negative\n",
            "10  positive   Neutral\n",
            "11  positive   Neutral\n",
            "12  positive  Negative\n",
            "13  positive  Negative\n",
            "14  positive   Neutral\n",
            "15  positive  Negative\n",
            "16  negative   Neutral\n",
            "17  positive  Negative\n",
            "18  positive   Neutral\n",
            "19  positive   Neutral\n",
            "20  negative  Negative\n",
            "21  positive  Negative\n",
            "22  positive   Neutral\n",
            "23  positive  Negative\n",
            "24  positive   Neutral\n",
            "25  positive  Negative\n",
            "26  positive   Neutral\n",
            "27  positive   Neutral\n",
            "28  positive  Positive\n",
            "29  positive   Neutral\n",
            "30  positive  Positive\n",
            "31  negative  Positive\n",
            "32  positive   Neutral\n",
            "33  positive   Neutral\n",
            "34  positive   Neutral\n",
            "35  positive   Neutral\n",
            "36  positive   Neutral\n",
            "37  positive   Neutral\n",
            "38  positive   Neutral\n",
            "39  positive  Negative\n",
            "40  positive   Neutral\n",
            "41  positive  Negative\n",
            "42  positive   Neutral\n",
            "43  positive   Neutral\n",
            "44  positive  Positive\n",
            "45  positive  Positive\n",
            "46  positive  Negative\n",
            "47  positive  Negative\n",
            "48  positive   Neutral\n",
            "49  positive   Neutral\n",
            "         sen  pred sen\n",
            "58  positive  positive\n",
            "30  positive  positive\n",
            "40  positive  positive\n",
            "69  positive  positive\n",
            "97  positive  positive\n",
            "23  positive  positive\n",
            "44  positive  positive\n",
            "1   positive  positive\n",
            "90  positive  positive\n",
            "94  positive  positive\n",
            "35  positive  positive\n",
            "15  positive  positive\n",
            "92  positive  positive\n",
            "83  positive  positive\n",
            "89  positive  positive\n",
            "17  positive  positive\n",
            "96  positive  positive\n",
            "7   positive  positive\n",
            "75  positive  positive\n",
            "98  positive  positive\n",
            "1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qognF6WcyDHk"
      },
      "source": [
        "\"\"\"from above analysis out of all models svm resulted in better for sentiment analysis with 0.45 f1 score"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}